<p style="font-size:11px;"><em><strong>Créditos</strong>: El contenido de este cuaderno ha sido tomado de varias fuentes, pero especialmente de <a href="https://rpubs.com/jimclark/883880">Jim Clark</a>, <a href="https://cran.r-project.org/web/packages/CARBayes/vignettes/CARBayes.pdf">CARBayes</a>, <a href="https://search.r-project.org/CRAN/refmans/CARBayes/html/00Index.html">CRAN</a>, <a href="https://datascienceplus.com/spatial-regression-in-r-part-1-spamm-vs-glmmtmb/">Lionel Hertzog</a>, <a href="https://codingclubuc3m.rbind.io/post/2019-11-05/">Virgilio Gómez Rubio</a>. El compilador se disculpa por cualquier omisión involuntaria y estaría encantado de agregar un reconocimiento.</em></p>

# Modelos Lineales Jerárquicos Generalizados Espaciales

Los métodos basados en Modelos Lineales Generalizados (GLM, por sus siglas en inglés) son populares porque son fáciles de interpretar y extender. 
Los GLM abarcan una amplia clase de modelos donde se asume que la variable de respuesta ($y$) sigue una distribución de la familia exponencial 
(\cite{coxe2013generalized, hastie2017generalized}), como la Gaussiana, Poisson, Binomial o Gamma. El valor esperado de la respuesta ($\mu$) 
se modela mediante un predictor lineal ($\eta$) a través de una función de enlace ($g(\mu)=\eta$) que asume efectos aditivos (o pesos) de las 
covariables. Los GLM tienen numerosas ventajas sobre otros métodos estadísticos, pueden acomodar covariables tanto continuas como discretas, y 
permiten la interpretación directa de los coeficientes del modelo (\cite{camilo2017handling, goetz2011integrating}).

La regresión logística ha sido un método GLM ampliamente utilizado para modelar utilizando la función de enlace Logit para mapear la combinación 
lineal de predictores $x_i^T \beta$ a valores del intervalo unitario, que se interpretan como probabilidades de éxito bajo una distribución de 
Bernoulli (\cite{david2000applied}). Sin embargo, la regresión logística puede ser inconsistente en diferentes resoluciones espaciales. 
La regresión de Poisson es otro miembro de los GLM que modela datos de conteo, permitiendo la agregación de cuentas y probabilidades a lo largo 
de cualquier unidad de mapeo. La distribución de Poisson tiene un solo parámetro, $\lambda$, que es tanto la media como la varianza.

\begin{align}
Y \sim Poisson(\lambda), E(Y)=\lambda, \\
var(Y)=\lambda, \\
\lambda=log(\eta), \\
\eta=\alpha+\sum_{j=1}^p\beta_jx_{ij}+log(A),
\end{align}

donde $\eta_i$ es el predictor lineal, $\alpha$ es el intercepto, $x_{ij}$ es el $i$-ésimo nivel de la $j$-ésima covariable, hay $p$ covariables, 
$\beta$ son los parámetros de efecto lineal, y $\log(A)$ es un offset utilizado para ajustar las diferencias en el tamaño ($A$) de la unidad 
de mapeo.

Los GLM se pueden extender a Modelos Lineales Generalizados Jerárquicos (HGLM, por sus siglas en inglés) (\cite{lee1996hierarchical, lee2017data}), 
donde los coeficientes del modelo se dividen en efectos fijos (o a nivel de población) y efectos aleatorios (o a nivel de grupo). Los efectos 
aleatorios se refieren a parámetros que no son el enfoque principal del estudio pero que pueden influir en la variable objetivo, por lo que necesitan 
incluirse en el modelo (\cite{firebaugh2013fixed}). Los efectos fijos, por otro lado, son los predictores principales de interés en el estudio 
(\cite{torres2007panel}).

\[
\begin{aligned}
\eta_i =g(\mu_i)=\underbrace{\alpha+\sum_{j=1}^p\beta_jx_{ij}}_\text{efectos fijos}+\underbrace{\sum_{l=1}^r\gamma_l(z_{il})+\varepsilon_i}_\text{efectos aleatorios}
\end{aligned}
\]

donde: $g(.)$ es la función de enlace, $z_{il}$ son los (i-ésimo nivel del l-ésimo efecto aleatorio i.i.d.), $r$ efectos aleatorios estructurados, 
$\gamma_l$ son los $r$ efectos aleatorios estructurados, $\varepsilon_i$ son los residuos (efectos aleatorios no estructurados).

Aunque los HGLM se aplican frecuentemente a datos espaciales, no son necesariamente modelos espaciales (\cite{dong2015multilevel}). 
Aunque naturalmente capturan efectos espaciales dentro de niveles superiores y proporcionan un marco flexible para modelar la heterogeneidad 
espacial de los efectos de las covariables, estos modelos no logran capturar las estructuras de vecindad y la dependencia espacial 
(\cite{dong2015multilevel}). Por lo tanto, las unidades de nivel inferior, se asumen independientes entre sí, incluso cuando son vecinas 
(\cite{rabe2008multilevel}). Así, los HGLM deben tener en cuenta las correlaciones entre los efectos aleatorios ubicados en proximidad geográfica 
cercana (\cite{osland2016accounting}). De este modo, los efectos aleatorios pueden especificarse como un término aleatorio estructurado 
espacialmente, e interpretarse como los efectos de procesos no observados con estructura espacial (\cite{swanson2013spatial}). 
El efecto espacial latente representa el componente no explicado, aunque estructurado espacialmente, de los datos que permanece después de tener 
en cuenta los efectos de las covariables (\cite{austin2013covariate}). Las variables latentes son no observadas y no pueden medirse directamente, 
pero pueden inferirse a partir de los datos (\cite{aigner1984latent}).

## Campos Aleatorios de Markov

La estructura espacial de los datos discretos se modela principalmente utilizando dos enfoques (\cite{cao2014spatial}). El primero consiste en 
tratar los datos como continuos: los datos de cada región discreta se proyectan a su centroide geográfico como observaciones referenciadas a puntos 
dentro de una superficie espacial (o campo) y se modelan con modelos de Procesos Gaussianos (\cite{vasudevan2009gaussian}), incluido el conocido 
método de kriging, para la predicción espacial (\cite{cressie1988spatial}). La estructura espacial se representa mediante una matriz de covarianza 
basada en las distancias por pares entre centroides. Por lo tanto, los Procesos Gaussianos son computacionalmente costosos para grandes conjuntos 
de datos con matrices de covarianza proporcionalmente grandes (\cite{banerjee2008gaussian}). El segundo enfoque, que utilizamos aquí, aprovecha la 
naturaleza de índice discreto de los datos areales. La estructura espacial se representa mediante una matriz de conectividad de vecindad 
(\cite{rodrigues2012bayesian}), que determina si las celdas son vecinas según métricas como contigüidad, distancia o vecinos más cercanos 
(\cite{moraga2023spatial}). Las dos estructuras de conectividad de vecindad más comunes son los modelos autorregresivos simultáneos y condicionales 
(SAR y CAR) (\cite{jaya2021spatial}). En los modelos SAR, el efecto de rezago espacial se obtiene de la variable dependiente, y el término de error 
o las covariables se incorporan al modelo como un nuevo término (\cite{hooten2014simultaneous}). En cambio, los modelos CAR con Campos Aleatorios de 
Markov (MRF) se utilizan ampliamente en estadísticas espaciales para modelar datos observados, así como variables latentes y efectos aleatorios que 
varían espacialmente (\cite{earnest2007evaluating}). Las estructuras CAR se implementan en modelos jerárquicos con efectos aleatorios latentes 
(\cite{schmidt2014conditional}). CAR es un tipo de Campo Aleatorio de Markov (MRF) (\cite{besag1991bayesian}), que es el enfoque dominante para 
analizar datos espaciales discretos dentro de un marco jerárquico. Un proceso espacial se considera que tiene propiedades de Markov si el estado 
futuro esperado de un parámetro depende únicamente del estado adyacente inmediato, haciendo que el estado futuro sea condicionalmente independiente. 
Extender esto a múltiples efectos aleatorios da como resultado un MRF (\cite{clifford1990markov, rue2005gaussian}). La forma más común de MRF 
representa una conectividad de vecindad de primer orden en términos de contigüidad, donde las TMU que comparten un límite se consideran vecinas.

Varios modelos han sido propuestos dentro de esta clase general de estructuras CAR, incluidos los modelos intrínsecos y de convolución 
(\cite{besag1991bayesian}), así como alternativas como la propuesta por \cite{leroux2000estimation}. El modelo CAR intrínseco (ICAR) es el 
CAR más simple, que asume un efecto aleatorio autorregresivo espacial para abordar las asociaciones entre regiones vecinas (\cite{besag1991bayesian}).
 La expectativa condicional es igual a la media de los efectos aleatorios en las unidades de mapeo de terreno vecinas, mientras que la varianza 
 condicional es inversamente proporcional al número de sus vecinos. Este modelo representa estructuras de correlación espacial fuertes y puede 
 no ser adecuado si los datos están débilmente correlacionados.

\cite{besag1991bayesian} propuso el modelo de convolución, o modelo Besag-York-Mollié (BYM), que combina dos efectos aleatorios latentes: un efecto 
latente ICAR ($\rho$) y un efecto latente Gaussiano i.i.d. ($\sigma$). Sin embargo, los dos componentes de efectos aleatorios separados no se pueden 
identificar individualmente, y solo se puede identificar su suma.

\cite{leroux2000estimation} propuso utilizar un único efecto aleatorio en su lugar, que incluye un parámetro ($\rho$) para medir el nivel de 
correlación espacial entre las unidades de mapeo de terreno, tomando valores en el intervalo unitario [0-1]. Este modelo es una variación de los 
modelos BYM e ICAR. Al igual que el modelo ICAR, tiene un parámetro de efecto aleatorio espacial para cada TMU. Sin embargo, la distribución 
condicional incorpora características tanto de efectos aleatorios espaciales estructurados como no estructurados (del modelo BYM) en un solo 
parámetro ($\rho$). El modelo de Leroux generaliza el modelo ICAR y el modelo independiente (es decir, un modelo sin ningún efecto aleatorio 
espacial estructurado). Cuando $\rho=1$, se recupera el modelo ICAR; cuando $\rho=0$, se recupera el modelo independiente. Así, el modelo de 
Leroux busca equilibrar estos dos modelos estimando el valor de $\rho$.

## Modelos con libreria CARBayes

```{r}
library(spBayes)
library(maps)
library(RANN)
library(gjam)
library(CARBayes)
library(CARBayesdata)
library(mgcv)
```

```{r}
#### Set up a square lattice region
m <- 12
xEast  <- 1:m
xNorth <- 1:m
grid   <- expand.grid(xEast, xNorth)
n      <- nrow(grid)
plot( NULL, xlim = c(0, m), ylim = c(0, m), xlab='East', ylab='North' )
abline(v=grid[,1], h=grid[,2])
text(grid[,1] - .5, grid[,2] - .5, 1:n, cex=.8)
```

```         
Set up distance and neighbourhood (W, based on sharing a common border) matrices
```

```{r}
D <- W <- as.matrix(dist(grid))
W[W != 1] <- 0 
```

```{r}
Q <- 3
x <- matrix( rnorm(Q*n), n, Q )
x[,1] <- 1
x2    <- x[,2]
x3    <- x[,3]
beta  <- matrix( rnorm(Q), Q, 1)
sigma <- .1
```

```{r}
# simulated based on distance D
phi <- t( rmvn(1, rep(0,n), 1*exp(-0.1*D) ) )
y   <- x%*%beta + phi[,2] + rnorm(n, 0, sigma)
```

```{r}
form <- as.formula(y ~ x2 + x3)

## Gaussian model
gaussianModel <- S.CARleroux(formula = form, family  = 'gaussian', W = W, 
                             burnin = 20000, n.sample = 100000, thin = 10, verbose = F)
gaussianModel
```

```{r}
#autocorrelation parameter
plot( gaussianModel$samples$rho, bty = 'n' )
```

```{r}
#random effect
fv <- gaussianModel$fitted.values
mf <- min(fv)
cc <- fv - mf
ss <- seq(0, max(cc), length.out=10)
cc <- findInterval(cc, ss)

colM <- colorRampPalette( c("red","orange","blue"))
colm <- colM(10)

symbols(x=grid[,1], y=grid[,2], squares = cc*0+1, bg=colm[cc],
        fg=colm[cc],inches=F, xlab='East', ylab='North')
```

```{r}
#no gaussean
lambda <- exp(x%*%beta + phi[,2] + rnorm(n, 0, sigma))
y <- rpois(n, lambda)

poissonModel <- S.CARbym(formula=form, family="poisson",
                         W=W, burnin=20000, n.sample=100000, thin=10, verbose=F)
poissonModel
```

##Multilevel model

```{r}
#### Set up a square lattice region
x.easting <- 1:10
x.northing <- 1:10
Grid <- expand.grid(x.easting, x.northing)
K <- nrow(Grid)
```

```{r}
#### set up distance and neighbourhood (W, based on sharing a common border) matrices
distance <- as.matrix(dist(Grid))
W <-array(0, c(K,K))
W[distance==1] <-1 
```

```{r}
#### Generate the number of individuals per area and which individuals to which areas
n <- sample(5:30, K, replace=TRUE)
n.total <- sum(n)
ind.area.temp <- rep(1:K, n)
ind.area <- sample(ind.area.temp, n.total, replace=FALSE)
```

```{r}
#### Generate the covariates and response data
x1 <- rnorm(n.total)
x2 <- rnorm(n.total)
phi <- mvrnorm(n=1, mu=rep(0,K), Sigma=0.4 * exp(-0.1 * distance))
phi.extend <- phi[ind.area]
logit <- x1 + x2 + phi.extend
prob <- exp(logit) / (1 + exp(logit))
trials <- rep(50,n.total)
Y <- rbinom(n=n.total, size=trials, prob=prob)
```

```{r}
#### Run the model
formula <- Y ~ x1 + x2

#### Toy example for checking
model <- S.CARmultilevel(formula=formula, family="binomial", ind.area=ind.area,
                trials=trials, W=W, burnin=10, n.sample=50)

model
```

```{r}
#### Set up a square lattice region
x.easting <- 1:10
x.northing <- 1:10
Grid <- expand.grid(x.easting, x.northing)
K <- nrow(Grid)

#### set up distance and neighbourhood (W, based on sharing a common border) matrices
distance <- as.matrix(dist(Grid))
W <-array(0, c(K,K))
W[distance==1] <-1 	
	
#### Generate the covariates and response data
x1 <- rnorm(K)
x2 <- rnorm(K)
theta <- rnorm(K, sd=0.05)
phi <- mvrnorm(n=1, mu=rep(0,K), Sigma=0.4 * exp(-0.1 * distance))
logit <- x1 + x2 + theta + phi
prob <- exp(logit) / (1 + exp(logit))
trials <- rep(50,K)
Y <- rbinom(n=K, size=trials, prob=prob)


#### Run the BYM model
formula <- Y ~ x1 + x2
## Not run: model <- S.CARbym(formula=formula, family="binomial", trials=trials,
#W=W, burnin=20000, n.sample=100000)
## End(Not run)

#### Toy example for checking
model <- S.CARbym(formula=formula, family="binomial", trials=trials,
W=W, burnin=20, n.sample=50)

model
```

## Datos areales con INLA (Markov)

Para ilustrar cómo se ajustan los modelos espaciales con INLA, se utilizará el conjunto de datos de leucemia de Nueva York. Éste ha sido ampliamente analizado en la literatura (ver, por ejemplo, Waller y Gotway, 2004) y está disponible en el paquete `DClusterm`. El conjunto de datos registra una serie de casos de leucemia en el norte del estado de Nueva York a nivel de distrito censal. Algunas de las variables en el conjunto de datos son:

-   **Casos:** Número de casos de leucemia en el período 1978-1982.
-   **POP8:** Población en 1980.
-   **PCTOWNHOME:** Proporción de personas que son propietarias de su vivienda.
-   **PCTAGE65P:** Proporción de personas de 65 años o más.
-   **AVGIDIST:** Distancia inversa promedio al sitio de tricloroetileno (TCE) más cercano.

Tenga en cuenta que el interés se centra en la exposición al TCE, utilizando AVGIDIST como un indicador de exposición. Las variables PCTOWNHOME y PCTAGE65P actuarán como posibles factores de confusión que deben incluirse en el modelo. Sin embargo, no lo haremos aquí porque queremos probar cómo los efectos espaciales latentes capturan la variación espacial residual.

El conjunto de datos se puede cargar de la siguiente manera:

```{r}
library(spdep)
library(DClusterm)
data(NY8)
```

Dado que el interés se centra en estudiar el riesgo de leucemia en el norte del estado de Nueva York, primero se calcula el número esperado de casos. Esto se hace calculando la tasa de mortalidad general (total de casos dividido por la población total) y multiplicándola por la población:

```{r}
rate <- sum(NY8$Cases) / sum(NY8$POP8)
NY8$Expected <- NY8$POP8 * rate
```

Una vez que se obtiene el número esperado de casos, se puede obtener una estimación aproximada del riesgo con la razón de mortalidad estandarizada (SMR), que se calcula como el número de casos observados dividido por el número de casos esperados.

```{r}
NY8$SMR <- NY8$Cases / NY8$Expected
```

En epidemiología, es importante producir mapas para mostrar la distribución espacial del riesgo relativo. En este ejemplo, nos centraremos en la ciudad de Syracuse para reducir el tiempo de cómputo necesario para producir el mapa. Por lo tanto, creamos un índice con las áreas de la ciudad de Syracuse:

```{r}
# Subset Syracuse city
syracuse <- which(NY8$AREANAME == "Syracuse city")
```

Un mapa de enfermedades se puede crear simplemente con la función `spplot` (del paquete `sp`):

```{r}
library(viridis)
spplot(NY8[syracuse, ], "SMR", #at = c(0.6, 0.9801, 1.055, 1.087, 1.125, 13),
   col.regions = rev(magma(16))) #gray.colors(16, 0.9, 0.4))
```

### Modelos mixtos lineales

El primer modelo que consideraremos es un modelo Gausiano sin efectos aleatorios latentes, ya que proporcionará una línea base para comparar con otros modelos. Para ajustar el modelo con INLA, se utiliza la función `inla`:

```{r}
library(INLA)

```

```{r}
m1 <- inla(Cases ~ 1 + AVGIDIST,
   data = as.data.frame(NY8),
   family = "Gaussian",verbose=T,
   E = NY8$Expected, control.predictor = list(compute = TRUE),
   control.compute = list(dic = TRUE, waic = TRUE))

summary(m1)
NY8$FIXED.EFF <- m1$summary.fitted[, "mean"]
```

### Regresión con efectos aleatorios

Se pueden agregar efectos aleatorios latentes al modelo para tener en cuenta la sobredispersión incluyendo efectos aleatorios Gaussianos i.i.d. en el predictor lineal. Para ajustar el modelo con INLA, primero se crea un índice para identificar los efectos aleatorios (ID). Los efectos aleatorios latentes se especifican con la función `f` en INLA:

```{r}
NY8$ID <- 1:nrow(NY8)
m2 <- inla(Cases ~ 1 + AVGIDIST + f(ID, model = "iid"),
  data = as.data.frame(NY8), family = "Gaussian", 
  E = NY8$Expected,
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE))

summary(m2)
NY8$IID.EFF <- m2$summary.fitted[, "mean"]
```

```{r}
spplot(NY8[syracuse, ], c("SMR", "FIXED.EFF", "IID.EFF"),
  col.regions = rev(magma(16)))
```

### Modelos espaciales para datos tipo poligonos

Los datos discretos (lattice o poligonos regulares o irregulares) involucran datos medidos en diferentes áreas, por ejemplo, vecindarios, ciudades, provincias, estados, etc. La dependencia espacial aparece porque las áreas vecinas mostrarán valores similares de la variable de interés.

Tenemos observaciones $y = {y_i}_{i=1}^n$ donde n es el número de áreas. A y se le asigna una distribución multivariante que tiene en cuenta la dependencia espacial. Una forma común de describir la proximidad espacial en datos discretos es mediante una matriz de adyacencia W. El elemento $W_{i,j}$ es distinto de cero si las áreas i y j son vecinas. Por lo general, dos áreas son vecinas si comparten un límite común.

La matriz de adyacencia se puede calcular utilizando la función `poly2nb` en el paquete `spdep`. Esta función considerará dos áreas como vecinas si sus bordes se tocan al menos en un punto (es decir, adyacencia de "Queen"):

```{r}
NY8.nb <- poly2nb(NY8)
```

Esto devolverá un objeto `nb` con la definición de la estructura del vecindario.

```{r}
NY8.nb
```

Además, estos objetos creados con la función `nb` se pueden visualizar gráficamente cuando se conoce información adicional sobre las áreas, como la ubicación de su centro.

```{r}
plot(NY8) 
plot(NY8.nb, coordinates(NY8), add = TRUE, pch = ".", col = "gray")
```

### Modelos de Regresión

A menudo, además de $y_i$, tenemos una serie de covariables $X_i$. Por lo tanto, es posible que deseemos regresionar $y_i$ en $X_i$. Además de las covariables, es posible que queramos tener en cuenta la estructura espacial de los datos. Se pueden utilizar diferentes tipos de modelos de regresión para modelar datos de celosía:

-   Modelos Lineales Generalizados (con efectos aleatorios espaciales)
-   Modelos de econometría espacial
-   Modelos Lineales Mixtos

Un enfoque común (para datos Gaussianos) es usar una regresión lineal con efectos aleatorios:

$Y = Xβ + Zu + ε$

El vector de efectos aleatorios $u$ se modela como una distribución Normal multivariante:

$u ∼ N(0, σ_u^2 Σ)$

$Σ$ se define de manera que induzca una mayor correlación con las áreas adyacentes. Z es una matriz de diseño para los efectos aleatorios y $ε_i ∼ N(0, σ^2)$, i = 1, ..., n es un término de error.

Los Modelos Lineales Mixtos Generalizados se pueden definir de manera similar utilizando una probabilidad diferente y vinculando el parámetro apropiado al predictor lineal.

Hay muchas formas diferentes de incluir la dependencia espacial en $Σ$:

-   Autoregresivo simultáneo (SAR):

$Σ^{-1} = [(I - ρW)' (I - ρW)]$

-   Autoregresivo condicional (CAR):

$Σ^{-1} = (I - ρW)$

-   CAR intrínseco (ICAR):

$Σ^{-1} = diag(n_i) - W$

$n_i$ es el número de vecinos del área i. $Σ_{i,j}$ depende de una función de d(i,j). Por ejemplo:

$Σ_{i,j} = exp{-d(i,j) / φ}$

-   "Mezcla" de matrices (modelo de Leroux et al.):

$Σ = [(1 - λ)I_n + λM]^{-1}; λ ∈ (0,1)$

M es la precisión de la especificación ICAR

Las especificaciones CAR e ICAR se han propuesto dentro del campo de la Estadística, mientras que la especificación SAR se acuñó dentro de la Econometría Espacial. Independientemente de su origen, todas las especificaciones aquí presentadas pueden considerarse efectos latentes Gaussianos con una matriz de precisión particular.

### Modelo ICAR

El primer ejemplo se basará en la especificación ICAR. Tenga en cuenta que el efecto latente espacial se define usando la función f. Esto requerirá un índice para identificar los efectos aleatorios en cada área, el tipo de modelo y la matriz de adyacencia. Para ello, se utilizará una matriz dispersa.

```{r}
# Create sparse adjacency matrix
NY8.mat <- as(nb2mat(NY8.nb, style = "B"), "Matrix")
# Fit model
m.icar <- inla(Cases ~ 1 +  AVGIDIST + 
    f(ID, model = "besag", graph = NY8.mat),
  data = as.data.frame(NY8), E = NY8$Expected, family ="Gaussian",
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE))

summary(m.icar)
NY8$ICAR <- m.icar$summary.fitted.values[, "mean"]

```

### Modelo BYM (Besag, York y Mollié)

El modelo de Besag, York y Mollié incluye dos efectos aleatorios latentes: un efecto latente ICAR y un efecto latente Gaussiano i.i.d. No es necesario definir estos dos efectos latentes si se establece el argumento `model` como "bym" cuando se define el efecto aleatorio latente con la función `f`.

```{r}
m.bym = inla(Cases ~ 1 +  AVGIDIST + 
    f(ID, model = "bym", graph = NY8.mat),
  data = as.data.frame(NY8), E = NY8$Expected, family ="Gaussian",
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE))

summary(m.bym)
NY8$BYM <- m.bym$summary.fitted.values[, "mean"]
```

### Modelo de Leroux et al.

Este modelo se define utilizando una "mezcla" de matrices (modelo de Leroux et al.) para definir la matriz de precisión del efecto latente. Este modelo se implementa utilizando el efecto latente `generic1`, que utiliza la siguiente matriz de precisión:

$Σ^{-1} = 1 / τ (I_n - ρ * λ_max * C); ρ ∈ [0,1)$

En esta ecuación, C es una matriz y λ_max su autovalor máximo.

Para definir el modelo correcto, debemos tomar la matriz C de la siguiente manera:

$C = I_n - M; M = diag(n_i) - W$

Entonces, λ_max = 1 y:

$Σ^{-1} = 1 / τ (I_n - ρ * λ_max * C) = 1 / τ (I_n - ρ * (I_n - M)) = 1 / τ ((1 - ρ)I_n + ρM)$

Para ajustar el modelo, el primer paso es crear la matriz M.

```{r}
ICARmatrix <- Diagonal(nrow(NY8.mat), apply(NY8.mat, 1, sum)) - NY8.mat
Cmatrix <- Diagonal(nrow(NY8), 1) -  ICARmatrix
```

El modelo se ajusta como de costumbre con la función `inla`. Tenga en cuenta que la matriz C se pasa a la función `f` usando el argumento `Cmatrix`:

```{r}
m.ler = inla(Cases ~ 1 +  AVGIDIST +
    f(ID, model = "generic1", Cmatrix = Cmatrix),
  data = as.data.frame(NY8), E = NY8$Expected, family ="Gaussian",
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE))
summary(m.ler)
NY8$LEROUX <- m.ler$summary.fitted.values[, "mean"]

```

### Spatial Lag Model (SLM)

Este modelo incluye covariables y un proceso autoregresivo en la respuesta. R-INLA incluye un efecto latente experimental llamado `slm` para ajustar el siguiente modelo:

$x = (In - ρW)^{-1} (Xβ + e)$

Los elementos del modelo son:

-   W: Matriz de adyacencia estandarizada por filas.
-   ρ: Parámetro de autocorrelación espacial.
-   X: Matriz de covariables, con coeficientes β.
-   e: Errores i.i.d. Gaussianos con varianza σ\^2.

El efecto latente `slm` es experimental y se puede combinar con otros efectos en el predictor lineal. Para definir un modelo con el efecto latente `slm` se necesitan los siguientes elementos:

-   **X:** Matriz de covariables. Esta matriz contiene los valores de las variables explicativas para cada unidad muestral.
-   **W:** Matriz de adyacencia estandarizada por filas. Esta matriz describe la estructura espacial del modelo. Un elemento `W_ij` distinto de cero indica que las unidades muestrales `i` y `j` son vecinas, y su valor refleja la fuerza de la vecindad. La estandarización por filas garantiza que la suma de cada fila sea igual a 1.
-   **Q:** Matriz de precisión de los coeficientes β. Esta matriz define la varianza previa de los coeficientes estimados para las variables explicativas en el modelo.
-   **Rango de ρ:** Rango del parámetro de autocorrelación espacial `ρ`. Este parámetro controla la fuerza de la dependencia espacial en el modelo. A menudo, el rango de `ρ` se define en base a los autovalores de la matriz de adyacencia `W`.

```{r}
#X
mmatrix <- model.matrix(Cases ~ 1 + AVGIDIST, NY8)

#W
W <- as(nb2mat(NY8.nb, style = "W"), "Matrix")

#Q
Q.beta = Diagonal(n = ncol(mmatrix), x = 0.001)

#Range of rho
rho.min<- -1
rho.max<- 1
```

Los argumentos del efecto latente `slm` se pasan a través del argumento `args.sm`. En este caso, hemos creado una lista con el mismo nombre para mantener juntos todos los valores requeridos:

```{r}
#Arguments for 'slm'
args.slm = list(
   rho.min = rho.min ,
   rho.max = rho.max,
   W = W,
   X = mmatrix,
   Q.beta = Q.beta
)
```

Además, se deben establecer las funciones a priori para el parámetro de precisión τ y el parámetro de autocorrelación espacial ρ:

```{r}
#Prior on rho
hyper.slm = list(
   prec = list(
      prior = "loggamma", param = c(0.01, 0.01)),
      rho = list(initial=0, prior = "logitbeta", param = c(1,1))
)
```

La definición de la función a priori utiliza una lista con nombre y diferentes argumentos. El argumento `prior` define la distribución a priori que se usará, y el argumento `param` define los parámetros de dicha distribución.

En este caso:

-   Se asigna una función a priori gamma a la precisión, con parámetros 0.01 y 0.01.
-   Se asigna una función a priori beta al parámetro de autocorrelación espacial, con parámetros 1 y 1. Esto es equivalente a una distribución uniforme en el intervalo (1, 1).

```{r}
#SLM model
m.slm <- inla( Cases ~ -1 +
     f(ID, model = "slm", args.slm = args.slm, hyper = hyper.slm),
   data = as.data.frame(NY8), family = "Gaussian",
   E = NY8$Expected,
   control.predictor = list(compute = TRUE),
   control.compute = list(dic = TRUE, waic = TRUE)
)

summary(m.slm)
NY8$SLM <- m.slm$summary.fitted.values[, "mean"]

```

Los valores estimados de los coeficientes aparecen como parte de los efectos aleatorios.

```{r}
round(m.slm$summary.random$ID[47:48,], 4)

```

La autocorrelación espacial se informa en la escala interna (es decir, entre 0 y 1) y necesita ser reescalada.

```{r}
marg.rho.internal <- m.slm$marginals.hyperpar[["Rho for ID"]]
marg.rho <- inla.tmarginal( function(x) {
  rho.min + x * (rho.max - rho.min)
}, marg.rho.internal)

inla.zmarginal(marg.rho, FALSE)
```

```{r}
plot(marg.rho, type = "l", main = "Spatial autocorrelation")

```

### Resultados

```{r}
spplot(NY8[syracuse, ], 
  c("FIXED.EFF", "IID.EFF", "ICAR", "BYM", "LEROUX", "SLM"),
  col.regions = rev(magma(16))
)
```

## Modelos espaciales con spaMM

### Modelo no espacial

A continuación se presenta un ejemplo donde incluso si hay un patrón espacial en los datos, esto no significa que deban usarse modelos de regresión espacial. En algunos casos, los patrones espaciales en la variable respuesta son generados por patrones espaciales presentes en las covariables, como el gradiente de temperatura, la elevación, etc. Una vez que tomamos en cuenta el efecto de estas covariables, los patrones espaciales en la variable respuesta desaparecen.

```{r}
# load libraries
library(tidyverse)
library(gridExtra)
library(NLMR)
library(DHARMa)

# simulate a random temperature gradient
temp <- nlm_distancegradient(ncol = 100, nrow = 100, origin = c(1,10,1,10), rescale = TRUE)

# extract the temperature values at 100 random points
dat <- data.frame(x = runif(100,0,100), y = runif(100,0,100))
dat$temperature <- raster::extract(temp, dat)

# simulate tree height
dat$height <- 20 + 35 * dat$temperature + rnorm(100)

# plot spatial pattern in tree height
ggplot(dat, aes(x = x, y = y, size = height)) +
  geom_point() +
  scale_size_continuous(range = c(1,10))
```

```{r}
# fit a non-spatial model
m_non <- lm(height ~ temperature, dat)
```

```{r}
# formal test
sims <- simulateResiduals(m_non)
testSpatialAutocorrelation(sims, x = dat$x, y = dat$y, plot = FALSE)
```

### Modelo espacial

```{r}
library(geoR)
library(viridis)
data(ca20)
# put this in a data frame
dat <- data.frame(x = ca20$coords[,1], y = ca20$coords[,2], calcium = ca20$data, elevation = ca20$covariate[,1], region = factor(ca20$covariate[,2]))

# plot the data
ggplot(dat, aes(x=x, y = y, color =calcium, shape = region)) +
  geom_point() +
  scale_color_viridis(option = "A")
```

```{r}
# fit a no-spatial model
m_lm <- lm(calcium ~ elevation + region, dat)
# test for spatial autocorrelation
sims <- simulateResiduals(m_lm)
testSpatialAutocorrelation(sims, dat$x, dat$y, plot = FALSE)
```

```{r}
library(spaMM)
# fit the model
m_spamm <- fitme(calcium ~ elevation + region + Matern(1 | x + y), data = dat, family = "gaussian") # this take a bit of time
# model summary
summary(m_spamm)
```

Hay dos salidas principales de interés aquí: la primera son los **efectos fijos (beta)**, que son los parámetros de regresión estimados (pendientes). Luego, el parámetro de correlación **nu** y **rho** que representan la fuerza y la velocidad de decaimiento del efecto espacial. Podemos convertir esto en el efecto de correlación espacial real al graficar la correlación estimada entre dos ubicaciones en función de su distancia.

Aquí desglosamos un poco más la terminología:

-   **Efectos fijos (beta):** En este contexto, los efectos fijos se refieren a los coeficientes estimados en la regresión lineal. Representan el cambio promedio en la variable respuesta por unidad de cambio en una variable predictora, suponiendo que todas las demás variables permanecen constantes.
-   **Parámetro de correlación nu (nu):** Nu es un parámetro que controla la fuerza de la autocorrelación espacial. Un valor de nu cercano a 1 indica una autocorrelación espacial fuerte, mientras que un valor cercano a 0 indica una autocorrelación espacial débil o inexistente.
-   **Parámetro de correlación rho (rho):** Rho es otro parámetro que controla la autocorrelación espacial, pero también afecta la velocidad a la que la correlación decae con la distancia. Un valor de rho positivo indica una autocorrelación espacial positiva (ubicaciones cercanas tienen valores similares), mientras que un valor negativo indica una autocorrelación espacial negativa (ubicaciones cercanas tienen valores diferentes). El valor absoluto de rho determina la rapidez con que la correlación decae con la distancia. Un valor cercano a 1 indica un decaimiento lento, mientras que un valor cercano a 0 indica un decaimiento rápido.

En resumen, el análisis proporciona información sobre la relación entre las variables predictoras y la variable respuesta, teniendo en cuenta la autocorrelación espacial. Los efectos fijos (beta) indican la fuerza de las relaciones lineales, mientras que los parámetros nu y rho cuantifican la autocorrelación espacial y su decaimiento con la distancia. Para visualizar completamente el efecto de correlación espacial, se recomienda graficar la correlación estimada entre dos ubicaciones en función de su distancia.

```{r}
dd <- dist(dat[,c("x","y")])
mm <- MaternCorr(dd, nu = 0.43, rho = 0.01)
plot(as.numeric(dd), as.numeric(mm), xlab = "Distance between pairs of location [in m]", ylab = "Estimated correlation")
```

```{r}
sims <- simulateResiduals(m_spamm)
plot(sims)
```

```{r}
# the effect of elevation
newdat <- expand.grid(x = 5000, y = 5200, elevation = seq(3, 7, length.out = 10), region = factor(1, levels = c(1:3)))

newdat$calcium <- as.numeric(predict(m_spamm, newdat, re.form = NA)) # re.form = NA used to remove spatial effects
newdat$calcium <- newdat$calcium + mean(c(0,fixef(m_spamm)[3:4])) # to remove region effect
# get 95% confidence intervals around predictions
newdat <- cbind(newdat, get_intervals(m_spamm, newdata = newdat, intervals = "fixefVar", re.form = NA) + mean(c(0,fixef(m_spamm)[3:4])))


gg1 <- ggplot(dat, aes(x = elevation, y = calcium)) +
  geom_point() +
  geom_path(data = newdat) +
  geom_ribbon(data = newdat, aes(ymin = fixefVar_0.025, ymax = fixefVar_0.975), alpha = 0.2)

# now for region effect
newdat <- data.frame(x = 5000, y = 5200, elevation = mean(dat$elevation), region = factor(1:3)) # averaging out elevation effect
newdat$calcium <- as.numeric(predict(m_spamm, newdat, re.form = NA))
# get 95% CI
newdat <- cbind(newdat,get_intervals(m_spamm, newdata = newdat, intervals = "fixefVar", re.form = NA))

gg2 <- ggplot(dat, aes(x = region, y = calcium)) +
  geom_jitter() +
  geom_point(data = newdat, color = "red", size = 2) +
  geom_linerange(data = newdat, aes(ymin = fixefVar_0.025, ymax = fixefVar_0.975), color = "red")

# plot together
grid.arrange(gg1, gg2, ncol = 2)
```

Ahora podemos predecir el efecto de la elevación y la región sobre la variable respuesta, teniendo en cuenta los efectos espaciales.

Esto significa que podemos utilizar el modelo de regresión espacial para estimar el impacto de la elevación y la región en la variable de respuesta, incluso después de considerar la autocorrelación espacial entre las mediciones. Al controlar los efectos espaciales, podemos aislar el efecto directo de la elevación y la región en la variable que estamos estudiando.

```{r}
library(fields)
library(raster)
# derive a DEM
elev_m <- Tps(dat[,c("x","y")], dat$elevation)
```

```{r}
r <- raster(xmn = 4950, xmx = 5970, ymn = 4800, ymx = 5720, resolution = 10)
elev <- interpolate(r, elev_m)

# for the region info use the limits given in ca20
pp <- SpatialPolygons(list(Polygons(list(Polygon(ca20$reg1)), ID = "reg1"),Polygons(list(Polygon(ca20$reg2)), ID = "reg2"), Polygons(list(Polygon(ca20$reg3)), ID = "reg3")))
region <- rasterize(pp, r)

# predict at any location
newdat <- expand.grid(x = seq(4960, 5960, length.out = 50), y = seq(4830, 5710, length.out = 50))
newdat$elevation <- extract(elev, newdat[,1:2])
newdat$region <- factor(extract(region, newdat[,1:2]))
# remove NAs
newdat <- na.omit(newdat)
# predict
newdat$calcium <- as.numeric(predict(m_spamm, newdat))

(gg_spamm <- ggplot(newdat,aes(x=x, y=y, fill = calcium)) +
  geom_raster() +
  scale_fill_viridis())
```
